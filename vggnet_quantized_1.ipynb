{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vggnet_quantized_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranavug/tp/blob/master/vggnet_quantized_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PvDNaHnhBSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " !git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjF1ZSabhe7i",
        "colab_type": "code",
        "outputId": "865e1853-73a8-4b5c-f4d9-a303eadbe57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd PyTorch_CIFAR10/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyTorch_CIFAR10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mDNsSjmhqwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip ../state_dicts.zip -d ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEdBFrM5ficf",
        "colab_type": "code",
        "outputId": "50666900-08c5-450f-ff42-3e72cae20279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import requests, zipfile, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def main():\n",
        "    url = \"https://rutgers.box.com/shared/static/y9wi8ic7bshe2nn63prj9vsea7wibd4x.zip\"\n",
        "\n",
        "    # Streaming, so we can iterate over the response.\n",
        "    r = requests.get(url, stream=True)\n",
        "\n",
        "    # Total size in Mebibyte\n",
        "    total_size = int(r.headers.get('content-length', 0))\n",
        "    block_size = 2**20 # Mebibyte\n",
        "    t=tqdm(total=total_size, unit='MiB', unit_scale=True)\n",
        "\n",
        "    with open('state_dicts.zip', 'wb') as f:\n",
        "        for data in r.iter_content(block_size):\n",
        "            t.update(len(data))\n",
        "            f.write(data)\n",
        "    t.close()\n",
        "\n",
        "    if total_size != 0 and t.n != total_size:\n",
        "        raise Exception('Error, something went wrong')\n",
        "        \n",
        "    print('Download successful. Unzipping file.')\n",
        "    path_to_zip_file = os.path.join(os.getcwd(), 'state_dicts.zip')\n",
        "    directory_to_extract_to = os.path.join(os.getcwd(), 'cifar10_models')\n",
        "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(directory_to_extract_to)\n",
        "        print('Unzip file successful!')\n",
        "        \n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.46G/2.46G [03:16<00:00, 12.5MMiB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Download successful. Unzipping file.\n",
            "Unzip file successful!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DFLL0L9fklb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.nn.modules.conv import _ConvNd\n",
        "from torch.nn.modules.utils import _single, _pair, _triple, _list_with_default\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "cfgs = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsO_W1VzgVa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=10, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQvn9m6Zk83c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, n_quants=64, beta=100):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.bias = bias\n",
        "        # @TODO: change initialization method to one mentioned\n",
        "        # in PyTorch implementation\n",
        "        l = 0.1\n",
        "        self.weight = torch.nn.Parameter(l*torch.randn(out_features, in_features))\n",
        "        self.bias = torch.nn.Parameter(l*torch.randn(out_features))\n",
        "        std = l * 1.\n",
        "        self.alpha = torch.nn.Parameter(torch.linspace(-2*l, 2*l, n_quants)) # quant params\n",
        "        self.beta = beta\n",
        "        # o = torch.sort(l*torch.randn(n_quants))\n",
        "        # print(o.values)\n",
        "        # self.alpha = torch.nn.Parameter(o.values)\n",
        "    def forward(self, input):\n",
        "        _, y = input.shape\n",
        "        beta = self.beta\n",
        "        #print(\"alpha\", self.alpha)\n",
        "        #print(\"orig\", self.weight[0])\n",
        "        virtual_alpha = self.alpha\n",
        "        virtual_weight = self.weight\n",
        "        virtual_bias = self.bias\n",
        "        virtual_weight = torch.unsqueeze(virtual_weight, -1)\n",
        "        virtual_weight = (virtual_weight - virtual_alpha)\n",
        "        # print(\"diff\", virtual_weight[0][0])\n",
        "        virtual_weight = virtual_weight\n",
        "        virtual_weight = -beta*virtual_weight*virtual_weight\n",
        "        # print(\"neg_sq\", virtual_weight[0][0])\n",
        "        # virtual_weight = torch.exp(virtual_weight)\n",
        "        virtual_weight = torch.nn.functional.softmax(virtual_weight, dim=2)\n",
        "        virtual_weight = virtual_weight * virtual_alpha\n",
        "        # print(virtual_weight.shape)\n",
        "        virtual_weight = torch.sum(virtual_weight, dim=2)\n",
        "        #print(virtual_weight.shape)\n",
        "        #print(\"final\", virtual_weight[0])\n",
        "        # output = input @ self.weight.t() + self.bias\n",
        "        output = input @ virtual_weight.t() + virtual_bias\n",
        "        return output\n",
        "\n",
        "class myConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1,\n",
        "                 bias=False, padding_mode='zeros',n_quants=64, beta=100):\n",
        "        temp = kernel_size\n",
        "        self.kernel_size = _pair(kernel_size)\n",
        "        self.stride = _pair(stride)\n",
        "        self.padding = _pair(padding)\n",
        "        self.dilation = _pair(dilation)\n",
        "        self.groups = groups\n",
        "        super(myConv2d, self).__init__()\n",
        "        l = 0.1\n",
        "        std = l * 1.\n",
        "        self.bias = None\n",
        "        self.alpha = torch.nn.Parameter(torch.linspace(-2*l, 2*l, n_quants)) # quant params\n",
        "        self.beta = beta\n",
        "        self.weight = torch.nn.Parameter(l*torch.randn(out_channels, in_channels,\n",
        "                                                     temp, temp))\n",
        "\n",
        "    def _conv_forward(self, input):\n",
        "        #print(weight.shape)\n",
        "        #_, y = input.shape\n",
        "        beta = self.beta\n",
        "        #print(\"alpha\", self.alpha)\n",
        "        #print(\"orig\", weight[0])\n",
        "        virtual_alpha = self.alpha\n",
        "        virtual_weight = self.weight.view(-1)\n",
        "        virtual_bias = self.bias\n",
        "        virtual_weight = torch.unsqueeze(virtual_weight, -1)\n",
        "        virtual_weight = (virtual_weight - virtual_alpha)\n",
        "        virtual_weight = -beta*virtual_weight*virtual_weight\n",
        "        # print(\"neg_sq\", virtual_weight[0][0])\n",
        "        # virtual_weight = torch.exp(virtual_weight)\n",
        "        virtual_weight = torch.nn.functional.softmax(virtual_weight, dim=1)\n",
        "        virtual_weight = virtual_weight * virtual_alpha\n",
        "        # print(virtual_weight.shape)\n",
        "        virtual_weight = torch.sum(virtual_weight, dim=1)\n",
        "        virtual_weight = virtual_weight.view(self.weight.shape)\n",
        "        #print(virtual_weight.shape)\n",
        "        #print(\"final\", virtual_weight[0])\n",
        "        # output = input @ self.weight.t() + self.bias\n",
        "        # output = input @ virtual_weight.t() + virtual_bias\n",
        "        return F.conv2d(input, virtual_weight, None, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "\n",
        "    def forward(self,input):\n",
        "        return self._conv_forward(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92rVOTyVk19F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=10, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            myLinear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            myLinear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            myLinear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = myConv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glbNCuPAgepI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _vgg(arch, cfg, batch_norm, pretrained, progress, device, **kwargs):\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        script_dir = os.path.dirname(__file__)\n",
        "        state_dict = torch.load(script_dir + '/state_dicts/'+arch+'.pt', map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def vgg16_bn(pretrained=False, progress=True, device='cpu', **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, device, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEr9KIbrgtDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python cifar10_train.py --classifier vgg16_bn --gpu '0,' --batch_size 256 --max_epochs 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKoUoTlb7pAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python cifar10_test.py --classifier vgg16_bn --gpu '0,' --batch_size 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqOsJh3DiSMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv state_dicts ./cifar10_models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7iFeyv-idl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pytorch-lightning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D-xxt1vM5EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.nn.modules.conv import _ConvNd\n",
        "from torch.nn.modules.utils import _single, _pair, _triple, _list_with_default\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "class myReLU(nn.Module):\n",
        "    def __init__(self, num_bits = 3):\n",
        "        super(myReLU, self).__init__()\n",
        "        self.alpha = torch.nn.Parameter(torch.tensor(10.0))\n",
        "        self.k = num_bits\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 0.5*(torch.abs(x) - torch.abs(x-self.alpha) + self.alpha)\n",
        "        out = torch.round(out*((2**self.k-1)/self.alpha))\n",
        "        out = out * ((self.alpha)/(2**self.k-1))\n",
        "        return out\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=10, init_weights=True):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            myReLU(6),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            myReLU(6),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), myReLU(6)]\n",
        "            else:\n",
        "                layers += [conv2d, myReLU(6)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "cfgs = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def _vgg(arch, cfg, batch_norm, pretrained, progress, device, **kwargs):\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        script_dir = os.path.dirname(__file__)\n",
        "        state_dict = torch.load(script_dir + '/state_dicts/'+arch+'.pt', map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg11_bn(pretrained=False, progress=True, device='cpu', **kwargs):\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, device, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg13_bn(pretrained=False, progress=True, device='cpu', **kwargs):\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, device, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg16_bn(pretrained=False, progress=True, device='cpu', **kwargs):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, device, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"VGG 19-layer model (configuration \"E\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def vgg19_bn(pretrained=False, progress=True, device='cpu', **kwargs):\n",
        "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, device, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}